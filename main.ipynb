{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_exer1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3jlW+3nV0qD8bEiZD1L4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcoswds/sentiment-analysis/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SMUMjq61Z3G"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv('imdb.csv',sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qvqx-q_S2F7d",
        "outputId": "5481ed1f-9301-421d-f441-92135407826c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review,sentiment</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    review,sentiment  ... Unnamed: 6\n",
              "0  One of the other reviewers has mentioned that ...  ...        NaN\n",
              "1  A wonderful little production. <br /><br />The...  ...        NaN\n",
              "2  I thought this was a wonderful way to spend ti...  ...        NaN\n",
              "3  Basically there's a family where a little boy ...  ...        NaN\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...        NaN\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V7A_KDdVbu-"
      },
      "source": [
        "#dropa colunas importadas erronemanete\r\n",
        "df.drop(['Unnamed: 1','Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6'],axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUdO-HCOVnPe"
      },
      "source": [
        "#renomeia a coluna importada para o nome correto\r\n",
        "df.rename(columns={'review,sentiment': 'review'},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOhz6I1iWk2b"
      },
      "source": [
        "#cria uma coluna do sentimento pegando a informação da ultima virgula até o final\r\n",
        "df['sentiment'] = df['review'].apply( lambda x: x.split(',')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45uuEhpIWyFQ"
      },
      "source": [
        "#retira a informação do sentimento da coluna review\r\n",
        "df['review'] = df['review'].apply( lambda x: x[:-1*(len(x.split(',')[-1]))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W57I-ZDueKLj"
      },
      "source": [
        "def limpar_lixo(x:str) -> str:\r\n",
        "\r\n",
        "  lixos = [\r\n",
        "    '<br /><br />'\r\n",
        "  ]\r\n",
        "\r\n",
        "  for lixo in lixos:\r\n",
        "    x = x.replace(lixo,'')\r\n",
        "\r\n",
        "  return x  \r\n",
        "\r\n",
        "df['review'] = df['review'].apply( lambda x: limpar_lixo(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MKZvmxPZAUs",
        "outputId": "e8363083-7f0e-47f9-ace5-f467759226fa"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download([\"stopwords\",\"punkt\",\"vader_lexicon\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoNRMBkRXze4"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "df['tokenized_text'] = df['review'].apply(word_tokenize) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2nuafEfZIMh"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "stopwords = stopwords.words('english')\r\n",
        "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [elem for elem in x if elem not in stopwords ]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x5ws_Dfh_1o"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\r\n",
        "stemmer = SnowballStemmer(\"english\")\r\n",
        "\r\n",
        "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [stemmer.stem(elem) for elem in x ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYd2ih6zDBtr"
      },
      "source": [
        "import numpy as np \r\n",
        "lst_col = 'tokenized_text'\r\n",
        "\r\n",
        "#Criando um row para cada palavra de cada lista do tokenized_text\r\n",
        "df_words = pd.DataFrame({\r\n",
        "      col:np.repeat(df[col].values, df[lst_col].str.len())\r\n",
        "      for col in df.columns.drop(lst_col)}\r\n",
        "    ).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHB_F59PEMqV"
      },
      "source": [
        "#Cria um index de 0 pra negativo e 1 para positivo de acordo com o sentimento\r\n",
        "df_words['sentiment_index'] = df_words['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\r\n",
        "#A coluna sentiment_index possui agora um valor variando de 0 a 1  para indicar se ela está mais relacionada com valores positivos ou negativos\r\n",
        "df_words = df_words.groupby(['tokenized_text'], as_index=False).agg({'sentiment_index': [np.mean]}) \r\n",
        "df_words.columns = list(map(''.join, df_words.columns.values))\r\n",
        "\r\n",
        "#Cria um dicionário do sentiment index para ser mais fácil a pesquisa\r\n",
        "dict_tst = df_words.set_index('tokenized_text')['sentiment_indexmean'].to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxTmgd_FQp9E"
      },
      "source": [
        "Muito da modelagem daqui para frente foi baseada em:\r\n",
        "https://realpython.com/python-nltk-sentiment-analysis/#training-and-using-a-classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g71q4dXe3eeL"
      },
      "source": [
        "from statistics import mean\r\n",
        "\r\n",
        "def find_positive_score(word):\r\n",
        "    try:\r\n",
        "      positive_score = dict_tst[word]\r\n",
        "    except:\r\n",
        "      positive_score = None\r\n",
        "    \r\n",
        "    return positive_score\r\n",
        "\r\n",
        "#função para usar com frases soltas\r\n",
        "def extract_features(text):\r\n",
        "    limpar_lixo(text)\r\n",
        "    words = word_tokenize(text)\r\n",
        "    words = [elem for elem in words if elem not in stopwords]\r\n",
        "    words = [stemmer.stem(elem) for elem in words ]\r\n",
        "    features = dict()\r\n",
        "    wordcount = 0\r\n",
        "    positive_scores = list()\r\n",
        "\r\n",
        "    for word in words:\r\n",
        "      if not find_positive_score(word)==None:\r\n",
        "        positive_scores.append(find_positive_score(word))\r\n",
        "\r\n",
        "    features[\"mean_positive\"] = mean(positive_scores)\r\n",
        "\r\n",
        "    return features\r\n",
        "\r\n",
        "#função para usar com tudo tratado no dataframe\r\n",
        "def extract_features_ready(words):\r\n",
        "    features = dict()\r\n",
        "    positive_scores = list()\r\n",
        "\r\n",
        "    for word in words:\r\n",
        "      if not find_positive_score(word)==None:\r\n",
        "        positive_scores.append(find_positive_score(word))\r\n",
        "\r\n",
        "    features[\"mean_positive\"] = mean(positive_scores)\r\n",
        "\r\n",
        "    return features    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlBdCXq8-eBs"
      },
      "source": [
        "features = [\r\n",
        "    (extract_features_ready(review), \"pos\")\r\n",
        "    for review in df[df['sentiment'] == 'positive'].tokenized_text.tolist() \r\n",
        "]\r\n",
        "features.extend([\r\n",
        "    (extract_features_ready(review), \"neg\")\r\n",
        "    for review in df[df['sentiment'] == 'negative'].tokenized_text.tolist() \r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT21Gs_UJvcg",
        "outputId": "f065d4e0-1afc-4ea7-bc86-46cc0fad8b9e"
      },
      "source": [
        "from sklearn.naive_bayes import (\r\n",
        "    BernoulliNB,\r\n",
        "    ComplementNB,\r\n",
        "    MultinomialNB,\r\n",
        ")\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\r\n",
        "from random import shuffle\r\n",
        "\r\n",
        "classifiers = {\r\n",
        "    \"BernoulliNB\": BernoulliNB(),\r\n",
        "    \"ComplementNB\": ComplementNB(),\r\n",
        "    \"MultinomialNB\": MultinomialNB(),\r\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier(),\r\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\r\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(),\r\n",
        "    \"LogisticRegression\": LogisticRegression(),\r\n",
        "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\r\n",
        "    \"AdaBoostClassifier\": AdaBoostClassifier(),\r\n",
        "}\r\n",
        "\r\n",
        "train_count = len(features) // 4\r\n",
        "shuffle(features)\r\n",
        "for name, sklearn_classifier in classifiers.items():\r\n",
        "  classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\r\n",
        "  classifier.train(features[:train_count])\r\n",
        "  accuracy = nltk.classify.accuracy(classifier, features[train_count:])\r\n",
        "  print(F\"{accuracy:.2%} - {name}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49.93% - BernoulliNB\n",
            "49.93% - ComplementNB\n",
            "49.93% - MultinomialNB\n",
            "92.11% - KNeighborsClassifier\n",
            "89.64% - DecisionTreeClassifier\n",
            "89.64% - RandomForestClassifier\n",
            "92.82% - LogisticRegression\n",
            "92.83% - MLPClassifier\n",
            "92.82% - AdaBoostClassifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiUNwAJMlThA",
        "outputId": "2d64ec98-fc50-4286-d9be-94eb1815d7a1"
      },
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\r\n",
        "sia = SentimentIntensityAnalyzer()\r\n",
        "\r\n",
        "def calculate_sia_score(review):\r\n",
        "  sia_score = sia.polarity_scores(review)\r\n",
        "  if sia_score['pos'] >= sia_score['neg']:\r\n",
        "    return 'positive'\r\n",
        "  else:\r\n",
        "    return 'negative'\r\n",
        "\r\n",
        "df['sia_sentiment'] = df['review'].apply(lambda x:calculate_sia_score(x)) \r\n",
        "df['sia_index'] = df.apply(lambda x: 1 if x['sia_sentiment'] == x['sentiment'] else 0,axis=1) \r\n",
        "sia_list = df['sia_index'].to_list()\r\n",
        "print(F\"{sum(sia_list)/len(sia_list):.2%} - SentimentIntensityAnalyzer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68.95% - SentimentIntensityAnalyzer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dj8pTm3nPL68",
        "outputId": "e1942d43-7f21-432a-8f7d-e94277b46337"
      },
      "source": [
        "new_review = \"I'm in a hurry\"\r\n",
        "f = extract_features(new_review)\r\n",
        "classifier.classify(f)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'neg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}